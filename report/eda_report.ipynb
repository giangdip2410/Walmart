{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDAレポート\n",
    "- 基本的にeda001とeda002をまとめたもの\n",
    "- Kernelは一切見てないです  \n",
    "\n",
    "## 目次\n",
    "1. Import\n",
    "2. Config\n",
    "3. DataLoad\n",
    "4. train、testに関する前提情報\n",
    "5. featuresに対する考察\n",
    "6. storesに関する考察\n",
    "7. Store、Deptに関する考察\n",
    "8. Weekly_Salesに関する考察\n",
    "9. 分析結果まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib_venn import venn2\n",
    "from ptitprince import RainCloud\n",
    "from pandarallel import pandarallel\n",
    "from datetime import datetime as dt\n",
    "import math\n",
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = '../input/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(INPUT+'train.csv')\n",
    "test = pd.read_csv(INPUT+'test.csv')\n",
    "features = pd.read_csv(INPUT+'features.csv')\n",
    "stores = pd.read_csv(INPUT+'stores.csv')\n",
    "sample = pd.read_csv(INPUT+'sampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#featuresとstoresをmerge\n",
    "train_feat_df = pd.merge(train,features.drop(columns='IsHoliday'),how='left',on=['Store','Date'])\n",
    "train_feat_df = pd.merge(train_feat_df,stores,how='left',on='Store')\n",
    "test_feat_df = pd.merge(test,features.drop(columns='IsHoliday'),how='left',on=['Store','Date'])\n",
    "test_feat_df = pd.merge(test_feat_df,stores,how='left',on='Store')\n",
    "\n",
    "#timestamp型に直す\n",
    "train['Date'] = train['Date'].parallel_apply(lambda x:dt.strptime(x,'%Y-%m-%d'))\n",
    "test['Date'] = test['Date'].parallel_apply(lambda x:dt.strptime(x,'%Y-%m-%d'))\n",
    "\n",
    "train['Year'] = train['Date'].dt.year\n",
    "train['Week'] = train['Date'].dt.week\n",
    "train['Month'] = train['Date'].dt.month\n",
    "test['Month'] = test['Date'].dt.month\n",
    "test['Year'] = test['Date'].dt.year\n",
    "test['Week'] = test['Date'].dt.week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train、testに関する前提情報\n",
    "- trainは2010-2012までのデータ、testは2012-2013のデータ → 未来を予測するコンペ\n",
    "- Dept、Store共に、unseenなものはない\n",
    "    - それぞれのStoreのそれぞれのDeptの未来を予測する\n",
    "- Weekly_Salesは毎週金曜に集計されている\n",
    "- データ量がかなり多い"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## featuresに対する考察\n",
    "- すべての特徴量とWeekly_Salesの相関がかなり低い → ノイズになるため使わない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = train_feat_df.corr()\n",
    "display(corr_mat['Weekly_Sales'].sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr_mat,cmap='Blues',annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#結構時間かかる\n",
    "columns = train_corr.drop(columns=['Store','Dept','Weekly_Sales'],axis=1).columns\n",
    "n_figs = len(columns)\n",
    "n_cols = 4\n",
    "n_rows = math.ceil(n_figs/n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(n_cols * 3, n_rows * 3), ncols=n_cols, nrows=n_rows)\n",
    "\n",
    "for c, ax in zip(columns, axes.ravel()):\n",
    "    sns.scatterplot(data=train_feat_df,x=c,y='Weekly_Sales',ax=ax)\n",
    "    ax.set_title(c)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## storesに関する考察\n",
    "- Size\n",
    "    1. 相関はないが、ノイズとまでは言い切れなさそう。特徴量として追加する\n",
    "    2. 散布図を見るとbinningしてカテゴリとして扱う方が良いこともわかる。\n",
    "- Type\n",
    "    1. Type毎にかなりWeekly_Salesに差があることがわかる。→特徴量として有効"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=train_feat_df,x='Size',y='Weekly_Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=train_feat_df,x='Type',y='Weekly_Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store、Deptに関する考察\n",
    "- StoreもDeptもそれぞれに違いはあるが、時系列としての動きはかなり似ている\n",
    "- Store、Dept毎にかなりWeekly_Salesに差がある\n",
    "- 各Storeで得意なDeptがある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StoreのDate時系列\n",
    "pivot_store =  pd.pivot_table(data=train,columns='Date',index='Store',values='Weekly_Sales')\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(pivot_store,ax=ax,cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DeptのDate時系列\n",
    "pivot_dept =  pd.pivot_table(data=train,columns='Date',index='Dept',values='Weekly_Sales')\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(pivot_dept,ax=ax,cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store毎に得意なDeptがないか見てみる\n",
    "pivot_dept_store =  pd.pivot_table(data=train,columns='Store',index='Dept'',values='Weekly_Sales')\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(pivot_dept_store,ax=ax,cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StoreのMonth時系列\n",
    "pivot_store =  pd.pivot_table(data=train,columns='Month',index='Store',values='Weekly_Sales')\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(pivot_store,ax=ax,cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DeptのMonth時系列\n",
    "pivot_store =  pd.pivot_table(data=train,columns='Month',index='Dept',values='Weekly_Sales')\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(pivot_store,ax=ax,cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekly_Salesに関する考察\n",
    "- 一年ごとの相関がかなり高い → 一年前のWeeklySalesを特徴量としてもかなり良い精度が出そう\n",
    "- IsHolidayは特別な場合にしか関係なさそう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weekly_SalesのDate時系列、うまいやり方がわからない。。教えてほしいです。。。\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sum_sales = train.groupby('Date')['Weekly_Sales'].sum().reset_index()\n",
    "sum_sales['IsHoliday'] = train.groupby('Date')['IsHoliday'].mean().reset_index(drop=True)\n",
    "sns.pointplot(data=sum_sales,x='Date',y='Weekly_Sales',hue='IsHoliday')\n",
    "sns.pointplot(data=sum_sales,x='Date',y='Weekly_Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#週ごとの合計時系列\n",
    "sales = pd.DataFrame(train.groupby(['Year','Week'])['Weekly_Sales'].sum()).reset_index()\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "sns.pointplot(data=sales,x='Week',y='Weekly_Sales',hue='Year')\n",
    "plt.title('Compair Weekly_Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.pivot_table(sales,index='Week',columns='Year',values='Weekly_Sales').corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析結果まとめ\n",
    "- 考察　一年前が大事！！\n",
    "    - データ量がかなり多い\n",
    "    - おそらくtrainとtestの分布はかなり似ている(adversal_validationで調べる)\n",
    "    - 時系列\n",
    "        - 時系列で見たときの各Store,各Deptの動きは一緒\n",
    "        - 一年毎の相関がかなり強い\n",
    "            - 週番号をカテゴリとしてみても\n",
    "        - 月ごとの関係はそこまで強くなさそう\n",
    "    - features\n",
    "        - ほとんど相関が無いのでまずは使わなくて良さそう\n",
    "    - stores\n",
    "        - Sizeはbinning,TypeはLabel or OneHot\n",
    "- Model  \n",
    "    1.Store,DeptをEmbeddingしてTransformerかLSTM(実装大変、時間かかる、過去の情報を上手く捉えてくれて一番精度が高そう)  \n",
    "        - トレンドが長いからTransformerのほうが強そう  \n",
    "    2.Dateを何週目という特徴にしてLightGBM(実装楽、時間そこそこ、精度そこそこ,過去の情報がRNNやARMAと比べて上手く使えなさそう？)  \n",
    "    3.統計モデル(ARMA等)も強そう。(実装が初めて、計算早そう、過去の情報から推測)\n",
    "\n",
    "- 特徴量 \n",
    "    - 一年の何週目か\n",
    "    - 移動平均\n",
    "    - 自己変化率\n",
    "    - n期前との差分\n",
    "    - 月,週ごとのSum_Sales\n",
    "    - クリスマスとか特別なイベントはフラグ立てる\n",
    "\n",
    "    - leakが怖いが、そのStore,Deptの一年前のWeekly_Salesをそのまま入れる"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
